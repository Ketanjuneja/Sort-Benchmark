CS553 PA2 :Sort on Hadoop/Spark 


Description:

The assignment is a bechmark for sorting data run via shared memory, Hadoop and Spark.The Shared memory part in run for 1,2,4 and 8 threads with 10 GB dataset.While the Hadoop and Spark are run for single node with 10 GB and multinode with 100 GB data respectively.


Files Included:

shared.jar			Tersort.jar			spark-sort.jar		raid.sh			server.c

make			start.sh			a.txt 				




Input to program:

The input to all program is generated using a 64-bit gensort which generates 100 bytes ascii records.For running the gensort program id done as follows:

./gensort -a <num-of-records> <input-file>

-a : stands for ascii character generation

<num-of-records>: Number of 100 byte ascii records

<input-file>: The path to store the records generated by gensort


Raid 0 Configuration:

The space on the installation (root) directory was very less hence additional EBS volumes were added to the existing instance and used as Raid 0 for faster access.The raid 0 is created with automated shell script "raid.sh".The shell script is run as 

./raid.sh


Parallel SSH Configuration

The multinode ssh connection are done using the pssh application which runs parallel connection to multiple nodes and run the shell commands 

pssh  -i  -h hosts.txt  -x "-oStrictHostKeyChecking=no  -i /path/to/key/something.pem" 'echo hi'

<hosts.txt> 

Installation , Execution and Explaination:



Shared Memory:

The program is developed in java and run as a jar file. A typical UNIX system with jvm installed can be run the program.The program along with all the libraries is stored in the file "shared.jar".The program can be executed as follows:

 java -jar shared.jar <block-size> <num_of_threads>

<block-size> : It is the number of records to be processed in memory . Each record is 100 bytes so if you want to process 1MB it would amount to 10000 records.

<num_of_threads> : It is the number of threads on which the sorting can be done n parallel.(tyically a multiple of 2)

Input/Output file: The default input file name is "input.txt" and default output file is "output.txt".
 
The program uses parallel external merge sort.The program can be divide into three phases:

1. Read phase: The input file is read as chunks of <block-size> and then divided into parts so that each thread can read its own part without affecting the other threads in parallel.

2. In-memory sort phase: In this each threads gets the input and sorts the input parallely using quick sort and writes it to temporary files. Each thread is provided its data from the read phase and sorts it accordingly.

3. Merge phase: The merge phase merges the output of the In-memory sort phase and generates the final output file.

 

Hadoop

The Hadoop program is written in Java and imports the standard org.hadoop libraries for the hadoop classes and methods.For runing the hadoop program the hadoop installation file needs to be downloaded from the apache mirrors.The hadoop is run in two mode single node with 10 GB data and 16 node with 100 GB data.

Single Node:

The Single node is run is pseudo distribute mode where the Hadoop deamons run as a seprate Java process.

Intial Config
The bashrc file is changed to update the path for Hadoop Config Java_home and Java_options (to mange heap size)

Files changed
core-site.xml: updating the path and port of hdfs
hdfs-site.xml: setting replication factor and permissions
mapred-site.xml: setting the jobtracker address.
slaves: changed to localhost

Multinode:

Intial config:
Same as single node remains same as single node for both master and slave

Master and Slave common files changed:
core-site.xml: updating the path and port of hdfs
hdfs-site.xml: setting replication factor and permissions
mapred-site.xml: setting the jobtracker address and mapreduce framework yarn.
master: IP address of master

Master files changed:
Slaves: this file contains IP address of all slaves and master's IP

Slaves files changed:
Slaves: this file contans its own IP address

Execution:

Intially run the ./start-all for hadoop/sbin which starts the hdfs and yarn

Upload input to hdfs (Or use Teragen ) 

Run command:
hadoop jar Terasort.jar <input-path> <output-path>

<input-path> : input path in HDFS 

<output-path> : output path path in HDFS 

Program:


The hadoop program consist of 3 main parts
1. Job Configuration : The job configuration takes the input output format for key/value pair the mapper class,reducer class.

2. Mapper: The mapper reads the input line by line and splits it into keyvalue pairs.The mapper splits the program into <String,String> key-value pair.

3. Reducer: The mapper output is shuffled and pre-sorted and hence the output of reducer is same as the input. 

    
    
   
Spark: 

The Spark program is written in Java and imports the standard org.spark libraries for the spark RDD and function.Spark is installed over the existing hadoop cluster.For runing the spark program the hadoop installation file needs to be downloaded from the apache mirrors and configured as mentioned above or spark can be run independently with its own dfs.The spark is run in two mode single node with 10 GB data and 16 node with 100 GB data.After making hadoop modification as told above the following modifications are to be done.

Modified files (common on master and slave)
spark-evn.sh: modify the local storage directory
Master: master file with IP address of master

Master node(modified files)
Slaves: IP address of all slaves

Slave nodes(modified files)
Slaves: IP address of itself

Execution:

Intially run the ./start-all for hadoop/sbin which starts the hdfs and yarn.
Then ./start-all to start executor and workers.

Upload input to hdfs. 

Run command:
./bin/spark-submit spark-sort.jar <input-path> <output-path>

<input-path> : input path in HDFS 

<output-path> : output path path in HDFS 

The Spark Program is done as follows:
1. Create Spark context to setup runtime configuration
2. Use the context to read input file.
3. Map the input file to JavaPairRDD
4. Perform sortbykey to sort the JavaPairRDD
5. Save the sorted RDD as Text File.

## Contributing

This benchmark is developed by Ketankumar Juneja
